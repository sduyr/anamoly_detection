{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Overview\n",
    "\n",
    "### What is Anomaly Detection?\n",
    "\n",
    "    Identifying patterns in data that deviate significantly from the norm (anomalies).\n",
    "    Examples: Fraudulent transactions, network intrusions, medical diagnosis.\n",
    "\n",
    "### Why Autoencoders?\n",
    "\n",
    "    Autoencoders excel at reconstructing inputs they were trained on (normal data).\n",
    "    Anomalies are harder to reconstruct, resulting in higher reconstruction errors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders : The Basics\n",
    "\n",
    "What is an Autoencoder?\n",
    "\n",
    "    A neural network architecture that learns to reconstruct its input.\n",
    "    Composed of:\n",
    "        Encoder: Compresses the input into a lower-dimensional latent space.\n",
    "        Decoder: Reconstructs the input from the latent representation.\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "    Measures the reconstruction error (e.g., Mean Squared Error).\n",
    "\n",
    "Why Reconstruction Error?\n",
    "\n",
    "    Normal data (training data) has low reconstruction errors.\n",
    "    Anomalies, being unfamiliar, yield higher reconstruction errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key metrics for Anomaly Detection\n",
    "\n",
    "Reconstruction Error:\n",
    "\n",
    "    Measures how well the autoencoder reconstructs the input.\n",
    "    Higher error → More likely an anomaly.\n",
    "\n",
    "Threshold:\n",
    "\n",
    "    A cutoff value to classify anomalies.\n",
    "    Determined using methods like the 95th percentile of reconstruction errors from normal data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Autoencoder\n",
    "\n",
    "    Forward Pass:\n",
    "        Input → Encoder → Latent Space → Decoder → Reconstructed Input.\n",
    "\n",
    "    Loss Function:\n",
    "        Reconstruction Loss= 1/ N ∑(x_original−x_reconstructed)^2\n",
    "        Measures how well the model reconstructs the input.\n",
    "\n",
    "    Optimization:\n",
    "        Use optimizers like Adam to minimize the reconstruction loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Process \n",
    "\n",
    "Step 1: Calculate Reconstruction Error:\n",
    "\n",
    "    Measure the error for each input in the test dataset.\n",
    "\n",
    "Step 2: Set a Threshold:\n",
    "\n",
    "    Use normal data to determine a threshold for classification.\n",
    "    Example: 95th percentile of reconstruction errors.\n",
    "\n",
    "Step 3: Classify Data:\n",
    "\n",
    "    Reconstruction Error > Threshold → Anomaly.\n",
    "    Reconstruction Error <= Threshold → Normal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics \n",
    "\n",
    "Precision:\n",
    "\n",
    "    Precision= True Positives/ (True Positives + False Positives) \n",
    "    How many detected anomalies are actually anomalies?\n",
    "\n",
    "Recall:\n",
    "\n",
    "    Recall=True Positives/ (True Positives + False Negatives) \n",
    "    How many anomalies were detected?\n",
    "\n",
    "F1-Score:\n",
    "\n",
    "    Harmonic mean of precision and recall.\n",
    "\n",
    "ROC-AUC:\n",
    "\n",
    "    Evaluates the trade-off between true positive rate and false positive rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset \n",
    "df = pd.read_csv('./data/creditcard.csv')\n",
    "scaler = MinMaxScaler()\n",
    "data = scaler.fit_transform(df.iloc[:,1:-1]) # Exclude the 'Class' column (labels)\n",
    "labels = df['Class'].values\n",
    "\n",
    "# Soplit data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Use only non-fradulent data training which is class = 0\n",
    "X_train = X_train[y_train== 0]\n",
    "X_test_normal =X_test[y_test== 0]\n",
    "X_test_anomalous = X_test[y_test== 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder (nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim),\n",
    "            nn.Sigmoid() # output between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded) \n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for Pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors \n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "X_test_normal_tensor = torch.tensor(X_test_normal, dtype=torch.float32)\n",
    "X_test_anomalous_tensor = torch.tensor(X_test_anomalous, dtype=torch.float32)\n",
    "\n",
    "# Data loader for training \n",
    "train_loader = torch.utils.data.DataLoader(X_train_tensor, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.0018\n",
      "Epoch [2/50], Loss: 0.0008\n",
      "Epoch [3/50], Loss: 0.0006\n",
      "Epoch [4/50], Loss: 0.0004\n",
      "Epoch [5/50], Loss: 0.0004\n",
      "Epoch [6/50], Loss: 0.0004\n",
      "Epoch [7/50], Loss: 0.0003\n",
      "Epoch [8/50], Loss: 0.0002\n",
      "Epoch [9/50], Loss: 0.0002\n",
      "Epoch [10/50], Loss: 0.0002\n",
      "Epoch [11/50], Loss: 0.0002\n",
      "Epoch [12/50], Loss: 0.0002\n",
      "Epoch [13/50], Loss: 0.0001\n",
      "Epoch [14/50], Loss: 0.0001\n",
      "Epoch [15/50], Loss: 0.0001\n",
      "Epoch [16/50], Loss: 0.0001\n",
      "Epoch [17/50], Loss: 0.0001\n",
      "Epoch [18/50], Loss: 0.0001\n",
      "Epoch [19/50], Loss: 0.0001\n",
      "Epoch [20/50], Loss: 0.0001\n",
      "Epoch [21/50], Loss: 0.0001\n",
      "Epoch [22/50], Loss: 0.0001\n",
      "Epoch [23/50], Loss: 0.0001\n",
      "Epoch [24/50], Loss: 0.0001\n",
      "Epoch [25/50], Loss: 0.0001\n",
      "Epoch [26/50], Loss: 0.0001\n",
      "Epoch [27/50], Loss: 0.0001\n",
      "Epoch [28/50], Loss: 0.0001\n",
      "Epoch [29/50], Loss: 0.0001\n",
      "Epoch [30/50], Loss: 0.0001\n",
      "Epoch [31/50], Loss: 0.0001\n",
      "Epoch [32/50], Loss: 0.0001\n",
      "Epoch [33/50], Loss: 0.0001\n",
      "Epoch [34/50], Loss: 0.0001\n",
      "Epoch [35/50], Loss: 0.0001\n",
      "Epoch [36/50], Loss: 0.0001\n",
      "Epoch [37/50], Loss: 0.0001\n",
      "Epoch [38/50], Loss: 0.0001\n",
      "Epoch [39/50], Loss: 0.0001\n",
      "Epoch [40/50], Loss: 0.0001\n",
      "Epoch [41/50], Loss: 0.0001\n",
      "Epoch [42/50], Loss: 0.0001\n",
      "Epoch [43/50], Loss: 0.0001\n",
      "Epoch [44/50], Loss: 0.0001\n",
      "Epoch [45/50], Loss: 0.0001\n",
      "Epoch [46/50], Loss: 0.0001\n",
      "Epoch [47/50], Loss: 0.0001\n",
      "Epoch [48/50], Loss: 0.0001\n",
      "Epoch [49/50], Loss: 0.0001\n",
      "Epoch [50/50], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function and optimizer\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = Autoencoder(input_dim)\n",
    "criterion = nn.MSELoss() # reconstruction loss \n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 50\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0 \n",
    "    for batch in train_loader:\n",
    "        # forward pass\n",
    "        reconstructed = model(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "\n",
    "        # backward pass \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97     85307\n",
      "         1.0       0.03      0.88      0.05       136\n",
      "\n",
      "    accuracy                           0.95     85443\n",
      "   macro avg       0.51      0.92      0.51     85443\n",
      "weighted avg       1.00      0.95      0.97     85443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Function to calculate reconstruction error\n",
    "def reconstruction_error(data, model):\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model(data)\n",
    "        error = torch.mean((reconstructed - data)**2, dim=1)    # Mean squared error per sample \n",
    "    return error.numpy()\n",
    "\n",
    "# calculate reconstruction error \n",
    "error_normal = reconstruction_error(X_test_normal_tensor, model)\n",
    "error_anomalous = reconstruction_error(X_test_anomalous_tensor, model)\n",
    "\n",
    "# Set a threshold for anomaly detection\n",
    "threshold = np.percentile(error_normal, 95) # 95th percentile of normal data \n",
    "\n",
    "# classify anomalies\n",
    "y_pred = np.concatenate([error_normal, error_anomalous]) > threshold\n",
    "y_true = np.concatenate([np.zeros(len(error_normal)), np.ones(len(error_anomalous))])\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class 0.0 (Normal transactions):\n",
    "\n",
    "    Precision = 1.00: All predicted normal transactions were correct.\n",
    "    Recall = 0.95: 95% of actual normal transactions were correctly identified.\n",
    "    F1-Score = 0.97: A high balance of precision and recall.\n",
    "    Support = 85,307: There are 85,307 normal transactions in the dataset.\n",
    "\n",
    "Class 1.0 (Fraudulent transactions):\n",
    "\n",
    "    Precision = 0.03: Out of all predicted frauds, only 3% were actually fraudulent.\n",
    "    Recall = 0.88: The model identified 88% of actual fraud cases.\n",
    "    F1-Score = 0.05: Low because precision is poor despite a good recall.\n",
    "    Support = 136: There are 136 fraudulent transactions in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
